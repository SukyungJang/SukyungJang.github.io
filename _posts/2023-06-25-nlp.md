---
layout: single
title:  "자연어 처리(NLP)에 대한 공부"
categories: Python
tags: [python, NLP, 자연어, nltk]
author_profile: false
toc: true
toc_sticky: true
toc_label: 목차
---

# 1. 자연어 처리란?

자연어(Natural Language)는 인간이 의사소통 및 정보 전달을 위해 사용하는 언어를 말합니다. 자연어는 우리 일상 생활에서 사용되는 언어로, 우리가 말하는 언어와 쓰는 언어를 모두 포함합니다. <br/>
<br/>

자연어는 사람들 사이에서 의미를 전달하고 이해하기 위한 기본적인 수단으로 사용됩니다. 우리는 자연어를 사용하여 아이디어, 감정, 지시사항, 질문 등을 전달하고 다른 사람들과 소통합니다. 일상적인 자연어의 형태로는 영어, 한국어, 중국어, 스페인어, 아랍어 등 다양한 언어들이 있습니다. <br/>
<br/>

자연어는 구조적이고 다의성이 있는 특징을 가지고 있습니다. 문장은 단어의 조합으로 구성되며, 문맥에 따라 단어의 의미와 해석이 달라질 수 있습니다. <br/>
<br/>

자연어 처리(Natural Language Processing, NLP)는 기계가 자연어를 이해하고 처리할 수 있는 기술 분야입니다. NLP는 컴퓨터가 텍스트 데이터를 이해하고 해석하여 정보를 추출하거나 자연어와 관련된 작업을 수행하는데 사용됩니다. 예를 들어, 기계 번역, 텍스트 분류, 감성 분석, 질의응답 시스템, 자동 요약 등은 NLP의 응용 분야에 해당합니다. <br/>
<br/>

자연어 처리는 텍스트 데이터의 전처리, 토큰화, 문법 및 의미 분석, 언어 모델링, 기계 학습 알고리즘 등 다양한 기술과 방법을 사용하여 이루어집니다. 최근에는 딥러닝과 같은 인공지능 기술의 발전으로 인해 자연어 처리의 성능이 크게 향상되었고, 실제로 많은 분야에서 활용되고 있습니다.

# 2. 자연어 처리의 활용
**반복 업무 자동화**: NLP로 구동되는 챗봇은 오늘날 사람이 하는 수많은 반복 작업을 처리할 수 있으며, 이를 통해 직원들이 보다 도전적이고 흥미로운 작업을 수행할 수 있도록 해줍니다. 예를 들어, 챗봇과 디지털 도우미는 다양한 사용자 요청을 인식한 다음 이를 기업 데이터베이스의 적절한 항목과 매칭하여 사용자에 대한 적절한 응답을 공식화할 수 있습니다. <br/>
<br/>

**검색 효율 향상**: NLP는 문맥에 기반하여 단어 의미를 명확히 하고(예:'캐리어'는 생체의학과 산업 분야의 문맥에서 다른 의미를 가짐), 동의어를 매칭시키고(예:'automobile' 검색 시 'car'가 언급된 문서 검색), 형태학적 변형을 고려(영어가 아닌 언어 질의와 관련하여 중요함)하여 문서에 대한 키워드 일치 검색 및 FAQ 검색 효율을 개선할 수 있습니다. 또한 효과적인 NLP 기반 학술 검색 시스템은 의사, 변호사 및 기타 전문가의 관련 최첨단 연구에 대한 접근성을 크게 향상시킬 수 있습니다. <br/>
<br/>

**검색 엔진 최적화**: NLP는 검색을 분석하여 콘텐츠를 최적화함으로써 기업의 온라인 검색 순위를 높이는데 유용한 도구입니다. 검색 엔진은 NLP를 사용하여 결과의 순위를 매기기 때문에 이러한 기술을 효과적으로 사용하는 방법을 알면 경쟁업체보다 쉽게 순위에 들 수 있으며 궁극적으로 비즈니스에 대한 가시성이 향상됩니다. <br/>
<br/>

**대규모 문서 컬렉션 분석 및 정리**: 문서 클러스터링 및 주제 모델링과 같은 NLP 기술은 기업 보고서나 뉴스 기사 또는 과학 문서와 같은 대규모 문서 컬렉션의 콘텐츠 다양성 파악 작업을 단순화시켜 줍니다. 이러한 기술은 법적 증거 수집 목적으로 자주 사용됩니다. <br/>
<br/>

**소셜 미디어 분석**: NLP는 고객의 리뷰와 소셜 미디어 댓글을 분석하여 방대한 양의 정보를 더 잘 파악할 수 있도록 해줍니다. 감정 분석 기술은 소셜 미디어의 댓글 스트림에서 긍정적인 댓글과 부정적인 댓글을 식별하여 고객의 감정을 실시간으로 직접 측정하는 것으로 고객 만족도와 매출 증가와 같은 막대한 수익으로 이어질 수 있습니다. <br/>
<br/>

**시장 통찰력**: NLP를 통해 비즈니스 고객의 언어를 분석하면 고객이 원하는 것을 보다 잘 파악하고 고객과의 소통 방법에 대한 더 나은 아이디어를 얻을 수 있습니다. 속성 중심(Aspect-oriented) 감정 분석은 소셜 미디어에서 특정 속성이나 제품과 관련된 감정을 감지하여 제품 디자인과 마케팅을 위한 직접 실행 가능한 정보를 제공합니다. <br/>
<br/>

**콘텐츠 조정**: 사용자 또는 고객 의견이 대량으로 접수되는 비즈니스의 경우 NLP를 사용하여 단어뿐만 아니라 어조와 의도까지 분석함으로써 기업이 전하는 메시지의 품질과 정중함이 유지될 수 있도록 내용을 조정할 수 있습니다. <br/>
<br/>

# 3. 자연어 처리 기술 개요

**NLP를 위한 머신러닝 모델**: 앞서 언급했듯이 현재의 NLP는 머신러닝이라는 AI 접근 방식에 크게 의존하고 있습니다. 머신러닝은 데이터 세트에 있는 예들을 일반화하여 예측을 수행하는데 이러한 데이터 세트를 학습 데이터라고 하며, 머신러닝 알고리즘은 이 학습 데이터에 대한 훈련을 통해 목표 작업을 수행하는 머신러닝 모델을 생성합니다. <br/>

예를 들어 감정 분석 훈련 데이터는 문장과 그 문장이 지닌 감정(예: 긍정적, 부정적 또는 중립적 감정)으로 구성됩니다. 머신러닝 알고리즘은 이러한 데이터 세트를 읽고 문장이 입력되면 그 문장의 감정을 반환하는 모델을 생성합니다. 이렇게 문장이나 문서가 입력되면 해당 입력값 대한 레이블을 반환하는 종류의 모델을 문서 분류 모델이라고 합니다. 또한 문서 분류기를 사용하여 언급된 주제(예: 스포츠, 금융, 정치 등)별로 문서를 분류할 수도 있습니다. <br/>

문서의 개체(entity)를 인식하고 분류하는데 사용되는 또 다른 모델도 있습니다. 이 모델은 문서의 각 단어에 대해 해당 단어가 개체 언급의 일부인지 여부와 그렇다면 어떤 개체가 관련되어 있는지를 예측합니다. 예를 들어, '어제 28달러에 거래된 XYZ사의 주식'이라는 문장에서 'XYZ사'는 회사 개체, '28달러'는 통화 금액, '어제'는 날짜입니다. 개체 인식을 위한 훈련 데이터는 텍스트의 모음이며 각 단어에는 해당 단어가 참조하는 개체의 종류가 표시됩니다. 이렇게 입력값의 각 단어에 대해 레이블을 생성하는 유형의 모델을 시퀀스 레이블링(Sequence Labeling Model)이라고 합니다. <br/>
<br/>

**시퀀스 투 시퀀스(Sequence-to-sequence)모델**은 NLP에서 사용되는 모델 제품군에 가장 최근에 추가되었습니다. 시퀀스 투 시퀀스(또는 seq2sqe)모델은 전체 문장이나 문서를 입력값으로 사용하지만 (문서 분류기에서와 같이)문장이나 다른 시퀀스(예: 컴퓨터 프로그램)를 출력값으로 생성합니다. (문서 분류기는 단일 기호만 출력값으로 생성) seq2seq 모델의 활용 사례에는 기계 번역도 포함되는데 영어 문장을 입력값으로 사용하고 프랑스어 문장을 출력값으로 반환하는 경우, 문서 요약(이 경우 출력값은 입력값의 요약), 의미 구문 분석(입력값은 영어로 된 질의 또는 요청, 출력값은 해당 요청을 실행하는 컴퓨터 프로그램) 경우 등이 그 예입니다. <br/>
<br/>

**딥 러닝, 사전 학습 모델, 전이 학습**: 딥 러닝은 NLP에서 가장 널리 사용되는 유형의 머신러닝입니다. 1980년대에 연구자들은 수많은 원시 머신러닝 모델을 하나의 네트워크로 결합한 신경망을 개발했는데 단순 머신러닝 모델을 뇌에 비유하여 '뉴런'이라고 부르곤 합니다. 이러한 뉴런은 계층적으로 배열되어 있으며 다층으로 구성된 네트워크를 심층 신경망이라고 합니다. 딥 러닝은 심층 신경망을 사용하는 머신러닝입니다. <br/>

일반적으로 심층 신경망은 그 복잡성으로 인해 훈련에 많은 학습 데이터가 필요하고 이를 처리하기 위한 높은 컴퓨팅 성능과 많은 시간이 필요합니다. 최신 심층 신경망 NLP 모델은 위키피디아의 모든 내용과 웹에서 스크랩한 데이터와 같은 다양한 소스에서 훈련됩니다. 훈련 데이터의 크기는 10GB 이상일 수 있으며 고성능 클러스터에서 심층 신경망을 훈련하려면 일주일 이상이 소요될 수 있습니다. (연구 결과 큰 데이터 세트에서 더 심층적인 모델을 훈련할수록 성능이 훨씬 더 높아진다는 사실이 확인됐으며, 이에 최근에는 점점 더 방대한 데이터셋에서 더 큰 모델을 훈련하려는 경쟁이 벌어지고 있습니다.) <br/>

심층 신경망에 요구되는 방대한 데이터와 컴퓨팅 성능은 그 유용성을 심각하게 제한하는 듯 합니다. 그러나 전이 학습을 통해 기존에 훈련된 심층 신경망을 추가적으로 훈련시키면 훨씬 더 적은 훈련 데이터와 컴퓨팅 성능으로도 새로운 작업을 수행하도록 할 수 있습니다. 가장 간단한 유형의 전이 학습을 미세 조정이라고 하는데 먼저 대규모의 일반 데이터 세트(예: 위키백과)에서 모델을 교육한 다음 실제 대상 작업으로 레이블이 지정된 훨씩 작은 작업별 데이터 세트에서 모델을 추가로 교육("미세 조정")하는 방식으로 구성됩니다. 놀랍게도 미세 조정 데이터 세트는 수백 또는 수십 개의 학습 사례만 포함된 매우 작은 규모일 수 있으며, 단일 CPU에서 미세 조정 훈련 시 몇 분밖에 걸리지 않습니다. 따라서 전이 학습을 사용하면 기업 전체에 딥 러닝 모델을 쉽게 배포할 수 있습니다. <br/>

현재는 다양한 언어와 데이터 세트, 사전 훈련 작업의 조합으로 훈련된 사전 학습 딥 러닝 모델을 제공하는 공급업체의 에코시스템이 구축되어 있으며 이러한 사전 학습 모델을 다운로드하여 다양한 목표 작업에 맞게 미세 조정할 수 있습니다.

# 3. NLP 전처리 기술
**토큰화**: 토큰화는 원시 텍스(예: 문장 또는 문서)를 단어 또는 하위 단어 조각과 같은 토큰 시퀀스로 분할하는 것을 말하며 NLP 처리 파이프라인의 첫 번째 단계인 경우가 많습니다. 토큰은 일반적으로 이후의 처리 과정에서 원자 단위로 처리되는 반복 텍스트 시퀀스로 단어, 형태소(예: 영어의 'un-'과 같은 접두사 또는 '-ing'과 같은 접미사)라고 하는 하위 단어 또는 개별 문자일 수 있습니다.


```python
# 샘플 코드
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize

# 문장 토큰화
text = "Hello, how are you? I'm doing well. How are you?"
sentences = sent_tokenize(text)
print(sentences)
```

    ['Hello, how are you?', "I'm doing well.", 'How are you?']
    


```python
# 단어 토큰화
words = word_tokenize(text)
print(words)
```

    ['Hello', ',', 'how', 'are', 'you', '?', 'I', "'m", 'doing', 'well', '.', 'How', 'are', 'you', '?']
    

**단어 주머니 모델(Bag-of-words models)**: 단어 주머니 모델은 문서를 정렬되지 않은 토큰 또는 단어 모음으로 취급합니다. (주머니는 각 요소가 출현하는 횟수를 추적한다는 점을 제외하면 세트와 같음) 단어 주머니 모델은 단어의 순서를 완전히 무시하기 때문에 '개가 사람을 문다(dog bites man)'와 '사람이 개를 문다(man bites dog)'같은 문장을 혼동하지만 검색 엔진과 같은 대규모 정보 검색 작업에서 효율성을 위해 종종 사용됩니다. 긴 문서를 사용하면 최첨단에 가까운 결과를 얻을 수 있습니다.


```python
# 샘플 코드
from sklearn.feature_extraction.text import CountVectorizer

# 문서 데이터
documents = [
    "I enjoy playing sports.",
    "I like to watch movies.",
    "I love eating pizza."
]

# CountVectorizer 객체 생성
vectorizer = CountVectorizer()

# 문서 데이터를 피쳐 벡터화
X = vectorizer.fit_transform(documents)

# 단어 주머니의 단어들 확인
feature_names = vectorizer.get_feature_names_out()
print(feature_names)
```

    ['eating' 'enjoy' 'like' 'love' 'movies' 'pizza' 'playing' 'sports' 'to'
     'watch']
    


```python
# 피처 벡터화된 행렬 확인
print(X.toarray())
```

    [[0 1 0 0 0 0 1 1 0 0]
     [0 0 1 0 1 0 0 0 1 1]
     [1 0 0 1 0 1 0 0 0 0]]
    

**불용어(Stop word) 제거**: '불용어'는 이후 처리 과정에서는 무시되는 토큰을 말하며, 일반적으로 'a'나 'an'또는 'an'과 같이 짧고 자주 사용되는 단어입니다. 단어 주머니 모델과 검색 엔진은 데이터베이스 내의 처리 시간과 저장 공간을 줄이기 위해 불용어를 무시하는 경우가 많습니다. 한편 심층 신경망은 일반적으로 단언 순서를 고려하며(즉, 단어 주머니 모델이 아님) 불용어가 미묘한 의미의 차이를 전달할 수 있기 때문에 제거하지 않습니다.(예: 'the package was lost'와 'a package is lost'는 불용어를 제거하면 남는 단어는 동일하지만 문장의 의미는 다름)


```python
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# 문장 데이터
sentence = "This is an example sentence demonstrating the removal of stopwords."

# NLTK에서 영어 불용어 목록 다운로드
nltk.download('stopwords')

# 영어 불용어 목록 가져오기
stop_words = set(stopwords.words('english'))

# 문장을 단어로 토큰화
words = word_tokenize(sentence)

# 불용어 제거
filtered_words = [word for word in words if word.lower() not in stop_words]

# 불용어가 제거된 단어 확인
print(filtered_words)
```

    ['example', 'sentence', 'demonstrating', 'removal', 'stopwords', '.']
    

    [nltk_data] Downloading package stopwords to
    [nltk_data]     C:\Users\82104\AppData\Roaming\nltk_data...
    [nltk_data]   Package stopwords is already up-to-date!
    

**어간 추출(Stemming) 및 표제어 추출(Lemmatization)**: 형태소는 뜻을 가진 가장 작은 말의 단위이며 일반적으로 단어보다 작습니다. 예를 들어, 'revisited'는 접두사 're-', 어간 'visit', 과거형 접미사 '-ed'로 구성됩니다. 어간 추출 및 표제어 추출은 단어를 어간 형태(예: 'revisit' + 과거형).로 매핑하는 것을 말하는데 사전 딥 러닝 모델에서는 중요한 단계이지만 딥 러닝 모델은 일반적으로 훈련 데이터에서 이러한 규칙성을 학습하므로 명시적인 어간 또는 형태소 추출 단계는 필요하지 않습니다.


```python
# 샘플 코드
import nltk
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.tokenize import word_tokenize
nltk.download('wordnet')

# 문장 데이터
sentence = "The cats are playing with each other and enjoying the sunshine."

# 토큰화
words = word_tokenize(sentence)

# Porter 어간 추출기
stemmer = PorterStemmer()

# 어간 추출
stemmed_words = [stemmer.stem(word) for word in words]

# 표제어 추출기
lemmatizer = WordNetLemmatizer()

# 표제어 추출
lemmatized_words = [lemmatizer.lemmatize(word) for word in words]

# 어간 추출 결과 확인
print(stemmed_words)
```

    [nltk_data] Downloading package wordnet to
    [nltk_data]     C:\Users\82104\AppData\Roaming\nltk_data...
    

    ['the', 'cat', 'are', 'play', 'with', 'each', 'other', 'and', 'enjoy', 'the', 'sunshin', '.']
    

**품사 태깅 및 구문 분석**: 품사(PoS, Part-of-speech) 태깅은 각 단어에 해당하는 품사(예: 명사, 동사, 형용사 등)로 레이블을 지정하는 프로세스입니다. 구문 분석가는 단어가 어떻게 결합되어 구와 절 그리고 전체 문장을 형성하는지 식별합니다. PoS 태깅은 시퀀스 레이블링 작업이고, 구문 분석은 확장된 종류의 시퀀스 레이블링 작업이며, 심층 신경망은 PoS 태깅과 구문 분석 모두를 위한 최첨단 기술입니다. 딥 러닝 이전에는 PoS 태깅과 구문 분석이 문장 이해에 필수적인 단계였지만 현재의 딥러닝 NLP 모델은 일반적으로 PoS 또는 구문 정보에서 얻을 수 있는 이익(있는 경우)이 미미하므로 딥 러닝 NLP에서는 PoS 태깅이나 구문 분석이 널리 사용되지 않습니다.


```python
import nltk

# 텍스트 예시
text = "I love eating pizza."

# 품사 태깅
tokens = nltk.word_tokenize(text)
pos_tags = nltk.pos_tag(tokens)
print("품사 태깅 결과:", pos_tags)
```

    품사 태깅 결과: [('I', 'PRP'), ('love', 'VBP'), ('eating', 'VBG'), ('pizza', 'NN'), ('.', '.')]
    


```python
# 구문 분석을 위한 문법 정의
grammar = nltk.CFG.fromstring("""
    S -> NP VP '.'
    NP -> PRP
    VP -> V NP
    V -> 'love' | 'eating'
    PRP -> 'I'
    NP -> 'pizza'
""")

# 구문 분석
parser = nltk.ChartParser(grammar)
trees = list(parser.parse(tokens))
print("구문 분석 결과:")
for tree in trees:
    print(tree)
```

    구문 분석 결과:
    

# 4. 참고 사이트
- Oracle: <https://www.oracle.com/kr/artificial-intelligence/what-is-natural-language-processing/>
- IBM: <https://www.ibm.com/kr-ko/topics/natural-language-processing>
- AWS: <https://aws.amazon.com/ko/what-is/nlp/>
